{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "analyzed-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, roc_auc_score, roc_curve, hamming_loss,precision_score,recall_score,f1_score\n",
    "\n",
    "from confusion import make_confusion_matrix\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['font.size'] = 17\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b055407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'../app/data/processed_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464331ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>rating</th>\n",
       "      <th>requirements</th>\n",
       "      <th>summary</th>\n",
       "      <th>job_title</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>salary</th>\n",
       "      <th>dateposted</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>synergisticit</td>\n",
       "      <td>at synergisticit, we aim to bring aboard it ...</td>\n",
       "      <td>alabama</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>collaborate with dynamic teams of engineers, d...</td>\n",
       "      <td>entry level data scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=57d47b0524890...</td>\n",
       "      <td>at synergisticit, we aim to bring aboard it ...</td>\n",
       "      <td>88000.0</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>alabama</td>\n",
       "      <td>alabama</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>synergisticit</td>\n",
       "      <td>about us:   synergistic it is a full-service s...</td>\n",
       "      <td>mountain brook, al 35223</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>undertaking machine learning experiments and t...</td>\n",
       "      <td>machine learning developer</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=e92afb112aad3...</td>\n",
       "      <td>about us:   synergistic it is a full-service s...</td>\n",
       "      <td>107000.0</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>al</td>\n",
       "      <td>mountain brook</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ierus technologies, inc.</td>\n",
       "      <td>ierus specializes in r&amp;d and low-rate producti...</td>\n",
       "      <td>huntsville, al 35805</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our applications include: radar, eo/ir, rf sig...</td>\n",
       "      <td>machine learning/artificial intelligence softw...</td>\n",
       "      <td>https://www.indeed.com/company/IERUS-Technolog...</td>\n",
       "      <td>ierus specializes in r&amp;d and low-rate producti...</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>al</td>\n",
       "      <td>huntsville</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>synergisticit</td>\n",
       "      <td>at synergisticit, we aim to bring aboard it ...</td>\n",
       "      <td>arkansas</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>collaborate with dynamic teams of engineers, d...</td>\n",
       "      <td>entry level data scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=91d02dbbb1961...</td>\n",
       "      <td>at synergisticit, we aim to bring aboard it ...</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>arkansas</td>\n",
       "      <td>arkansas</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indeed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phoenix, az</td>\n",
       "      <td>4.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our data scientists build and implement machin...</td>\n",
       "      <td>data science manager - job seeker profiles</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201000.0</td>\n",
       "      <td>2022-06-21</td>\n",
       "      <td>az</td>\n",
       "      <td>phoenix</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    company  \\\n",
       "0             synergisticit   \n",
       "1             synergisticit   \n",
       "2  ierus technologies, inc.   \n",
       "3             synergisticit   \n",
       "4                    indeed   \n",
       "\n",
       "                                         description  \\\n",
       "0    at synergisticit, we aim to bring aboard it ...   \n",
       "1  about us:   synergistic it is a full-service s...   \n",
       "2  ierus specializes in r&d and low-rate producti...   \n",
       "3    at synergisticit, we aim to bring aboard it ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                   location  rating requirements  \\\n",
       "0                   alabama     4.2          NaN   \n",
       "1  mountain brook, al 35223     4.2          NaN   \n",
       "2      huntsville, al 35805     4.7          NaN   \n",
       "3                  arkansas     4.2          NaN   \n",
       "4               phoenix, az     4.3          NaN   \n",
       "\n",
       "                                             summary  \\\n",
       "0  collaborate with dynamic teams of engineers, d...   \n",
       "1  undertaking machine learning experiments and t...   \n",
       "2  our applications include: radar, eo/ir, rf sig...   \n",
       "3  collaborate with dynamic teams of engineers, d...   \n",
       "4  our data scientists build and implement machin...   \n",
       "\n",
       "                                           job_title  \\\n",
       "0                         entry level data scientist   \n",
       "1                         machine learning developer   \n",
       "2  machine learning/artificial intelligence softw...   \n",
       "3                         entry level data scientist   \n",
       "4         data science manager - job seeker profiles   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.indeed.com/rc/clk?jk=57d47b0524890...   \n",
       "1  https://www.indeed.com/rc/clk?jk=e92afb112aad3...   \n",
       "2  https://www.indeed.com/company/IERUS-Technolog...   \n",
       "3  https://www.indeed.com/rc/clk?jk=91d02dbbb1961...   \n",
       "4  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "\n",
       "                                                text    salary  dateposted  \\\n",
       "0    at synergisticit, we aim to bring aboard it ...   88000.0  2022-06-20   \n",
       "1  about us:   synergistic it is a full-service s...  107000.0  2022-06-20   \n",
       "2  ierus specializes in r&d and low-rate producti...  120000.0  2022-06-20   \n",
       "3    at synergisticit, we aim to bring aboard it ...   90000.0  2022-06-20   \n",
       "4                                                NaN  201000.0  2022-06-21   \n",
       "\n",
       "      state            city  target  \n",
       "0   alabama         alabama     1.0  \n",
       "1        al  mountain brook     2.0  \n",
       "2        al      huntsville     2.0  \n",
       "3  arkansas        arkansas     1.0  \n",
       "4        az         phoenix     4.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c7eaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  at synergisticit, we aim to bring aboard it professionals to help them build a rewarding career in cutting-edge technologies. being in the industry for more than 10 years, we provide a splendid range of lucrative opportunities to sustain a position in our top tech clients like google, apple, cognizant, client, paypal, to name a few.   our seasoned team firmly believes that the new tech talent can scale any business if given the right opportunity. we value your integrity, hard work, and commitment to make a difference in the technical sphere. for this reason, we focus on providing end-to-end career assistance and enhancing your already existing it skills and knowledge.    currently, we are looking for qualified entry-level data scientists who can apply data science principles to design, test, implement, and develop data-based solutions, including reporting, auditing, and preparing large databases for statistical analysis.   minimum background and qualifications requirement bachelor's degree or master's degree in computer engineering, computer science, mathematics, electrical engineering, information systems, or it must have mathematics or statistics background technical and soft skills required experience in python programming and understanding of the software development life cycle. knowledge of linear algebra, statistics, and mathematics concepts. excellent written and verbal communication skills. highly motivated, self-learner, team player, and technically inquisitive. strong work ethics and creative problem-solving abilities. preferred skills deep learning data visualization nlp scala django roles and responsibilities collaborate with dynamic teams of engineers, developers, and scientists who research and integrate algorithms to develop an application, software, and computer system solutions to address complex data problems. assess project requirements and develop data analysis algorithms. engage developers to share their opinions, knowledge, and recommendations to meet the deliverables. contribute to technical solutions and implement software analyses to unlock the secrets held by big data sets. integrate components like web-based ui, commercial indexing products, and access control mechanisms to create operational information and knowledge discovery systems. benefits competitive salary flexible work schedule & part-time off e-verified no relocation h1b filing on job technical support skill enhancement opportunity to work with fortune 500 companies who should apply?  recent it graduates looking to build a solid career in the tech industry. if you're lured by the endless possibilities presented by ai, machine learning, iot, and data science, this job opportunity can be the right career path for you.  candidate's outcome : https://www.synergisticit.com/candidate-outcomes/   no third-party candidates or c2c candidates if you are interested, please apply to the posting. no phone calls please, shortlisted candidates would be reached out. collaborate with dynamic teams of engineers, developers, and scientists who research and integrate algorithms to develop an application, software, and computerâ€¦\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2618bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39c14552",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-906d49cccbb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7763\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7764\u001b[0m         )\n\u001b[0;32m-> 7765\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-906d49cccbb3>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mclean_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mclean_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[?|!|\\'|\"|#|/|-|,(|)|$|-|'\u001b[0m \u001b[0;34m'|:]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mr''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5461\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "def clean_text(data):\n",
    "    sentences = data.split('.')\n",
    "    clean_sentences = []\n",
    "    for i in sentences:\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|-|' '|:]',r'',i)\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|(0-9)]',r'',clean_sentence.strip(' '))\n",
    "        if len(clean_sentence.strip()) > 1:\n",
    "            clean_sentences.append(clean_sentence)\n",
    "    done_sent = ''\n",
    "    for i in clean_sentences:\n",
    "        done_sent += (' '+i)\n",
    "    return done_sent.strip()\n",
    "    \n",
    "\n",
    "data['cleaned'] = data[data.text.notnull()].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a78375",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = data[data.text.isna()]\n",
    "len(z)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee78eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in z.url:\n",
    "    print(i,'\\n')\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07269853",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.description.isna()].location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cdcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    sentences = data.split('.')\n",
    "    clean_sentences = []\n",
    "    for i in sentences:\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|-|' '|:]',r'',i)\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|-|' '|:]',r'',clean_sentence)\n",
    "        clean_sentence = \" \".join(re.findall(\"[(a-zA-Z,&)]+\", clean_sentence))\n",
    "        clean_sentences.append(clean_sentence)\n",
    "    clean_text = ''\n",
    "    for i in clean_sentences:\n",
    "        clean_text += (' '+i)\n",
    "    return clean_text.strip(' ')\n",
    "\n",
    "    \n",
    "\n",
    "data['cleaned'] = data.text.apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96973395",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cleaned.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e87e7",
   "metadata": {},
   "source": [
    "### 4.1 Target\n",
    "Before I can begin splitting the data I need to set the target for my methodology of training four seperate logistic regression models. I'm doing this because I'd like my classifications to be as accurate as possible, and also, by building my NLP strategy around a particular label, i.e. finding common words for that label as opposed to being generalized through the entire corpus\\\n",
    "\n",
    "I'm going to one-hot-encode the target feature so I can select each of the next columns as my y - one for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['company','job_title','state','city','rating','cleaned','target','salary', 'dateposted','summary','url']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, dtype='int')\n",
    "targets = ohe.fit_transform(pd.DataFrame(data.target))\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236fb12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.DataFrame(targets,columns=['Q1','Q2','Q3','Q4','unk'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea516b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(targets)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['unk'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "\n",
    "data['comment_text'] = data['cleaned'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404132b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.comment_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7348a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def stemming(sentence):\n",
    "    LemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = lemmatizer.lemmatize(word)\n",
    "        LemSentence += stem\n",
    "        LemSentence += \" \"\n",
    "    LemSentence = LemSentence.strip()\n",
    "    return LemSentence\n",
    "\n",
    "\n",
    "data['comment_text_lem'] = data['cleaned'].apply(stemming)\n",
    "data.comment_text_lem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960258da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['cleaned','comment_text'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b2661",
   "metadata": {},
   "source": [
    "I need to make one model for each target. Before doing so, I'll need to use gridsearch to find the best hyperperamters.\n",
    "Looping through each target, I set y to that specific single-column binary '(for target in targets target == 0 or 1)'\n",
    "Then do a test train split before setting up the pipeline. The grid being use in this case is the selection of hyperperamters I want to check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_pipe():\n",
    "    targets = ['Q1','Q2','Q3','Q4']\n",
    "    X = data.drop(targets, axis=1)\n",
    "\n",
    "    le_cols = ['company', 'job_title', 'state', 'city']\n",
    "    scal_cols = ['rating']\n",
    "\n",
    "    evaluations = {}\n",
    "\n",
    "    \n",
    "    for i in targets:\n",
    "    # test/train split\n",
    "        y = data[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y , test_size=.2, random_state=42)\n",
    "\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('text', TfidfVectorizer(ngram_range=(1,3), analyzer = 'word',max_features=1000,stop_words='english',decode_error='ignore'), 'comment_text_lem'),\n",
    "                ('category', OneHotEncoder(handle_unknown ='ignore'), le_cols),\n",
    "                ('scaler', StandardScaler(), scal_cols)\n",
    "                \n",
    "                ],remainder='drop')\n",
    "        #   populating parameter grid to search\n",
    "        grid = [\n",
    "            {\n",
    "            'classifier' : [LogisticRegression()],\n",
    "            #'classifier__penalty' : ['l1', 'l2'],\n",
    "            #'classifier__C' : np.logspace(-4, 4, 20),\n",
    "            'classifier__solver' : ['liblinear']}\n",
    "            ]\n",
    "\n",
    "        pipe = Pipeline(\n",
    "            steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(grid)),\n",
    "            ],\n",
    "            )\n",
    "\n",
    "\n",
    "        \n",
    "        grid_search = GridSearchCV(pipe, param_grid=grid, verbose=2, return_train_score=True)\n",
    "        grid_search.fit(x_train,y_train)\n",
    "\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "        print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "        if i not in evaluations.keys():\n",
    "            evaluations[i] = {\n",
    "                'grid_search':grid_search,\n",
    "                'x_train':x_train,\n",
    "                'y_train':y_train,\n",
    "                'x_test':x_test,\n",
    "                'y_test':y_test\n",
    "                \n",
    "            }\n",
    "\n",
    "\n",
    "    return evaluations\n",
    "\n",
    "\n",
    "evaluations = grid_search_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(evaluations):\n",
    "\n",
    "\n",
    "    targets = ['Q1','Q2','Q3','Q4']\n",
    "    X = data.drop(targets, axis=1)\n",
    "\n",
    "    test_results = {}\n",
    "\n",
    "    for i in targets:\n",
    "        evaluation = evaluations[i]['grid_search']\n",
    "        params = evaluation.best_params_\n",
    "\n",
    "\n",
    "        y = data[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y , test_size=.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "        le_cols = ['company', 'job_title', 'state', 'city']\n",
    "        scal_cols = ['rating']\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('text', TfidfVectorizer(ngram_range=(1,3), analyzer = 'word',max_features=1000,stop_words='english',decode_error='ignore'), 'comment_text_lem'),\n",
    "                ('category', OneHotEncoder(handle_unknown ='ignore'), le_cols),\n",
    "                ('scaler', MinMaxScaler(), scal_cols)\n",
    "                \n",
    "                ],remainder='drop')\n",
    "\n",
    "\n",
    "        pipe = Pipeline(\n",
    "            steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', params['classifier']),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "        pipe.fit(x_train,y_train)\n",
    "\n",
    "        predictions = pipe.predict(x_test)\n",
    "        score = pipe.score(x_train, y_train)\n",
    "\n",
    "\n",
    "        ### check\n",
    "        check = pd.DataFrame(preprocessor.fit_transform(x_train,y_train).toarray())\n",
    "\n",
    "\n",
    "        if i not in test_results.keys():\n",
    "            test_results[i] = pipe, x_train, y_train, x_test, y_test, predictions, score\n",
    "                \n",
    "    return test_results, check\n",
    "        \n",
    "test_results, check = make_model(evaluations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bccd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinator(test_results):\n",
    "    targets = ['Q1','Q2','Q3','Q4']\n",
    "\n",
    "    resultsDB = {}\n",
    "    lst = []\n",
    "\n",
    "    \n",
    "    for label in targets:\n",
    "        data_dict = {}\n",
    "        pipe = test_results[label][0]\n",
    "        x_test = test_results[label][3]\n",
    "        y_test = test_results[label][4]\n",
    "        y_prob = pipe.predict_proba(x_test)\n",
    "        predict_y = pipe.predict(x_test)\n",
    "        data_dict = {\n",
    "            f'{label}_y_test':y_test, \n",
    "             f'{label}_pred y': predict_y.tolist(),\n",
    "             f'{label}_probabilities': y_prob.tolist()\n",
    "             }\n",
    "        zulu = x_test.join(pd.DataFrame(data_dict))\n",
    "        if label not in resultsDB.keys():\n",
    "            resultsDB[label] = zulu\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "        out = pd.DataFrame(data_dict)\n",
    "        lst.append(out)\n",
    "\n",
    "    results_table = x_test\n",
    "    for i, label in zip(lst, targets):\n",
    "        results_table = results_table.join(i, lsuffix=label)\n",
    "    #results_table = results_table.join(lst[1], lsuffix='__')\n",
    "    #for label, results in zip(targets,lst):\n",
    "    #    results_table = results_table.join(results, lsuffix=label)\n",
    "    return resultsDB, results_table, lst\n",
    "   \n",
    "\n",
    "data_dict, results_table, lst = combinator(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_results['Q1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = test_results['Q1'][0]\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b937bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = test_results['Q1'][1]\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = test_results['Q1'][2]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc83433",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_results['Q1'][3]\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_results['Q1'][4]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  test_results['Q1'][5]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score =  test_results['Q1'][6]\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0377566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02645445",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pipe.score(x_train, y_train)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d80af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = pipe.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy :\",accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a401de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Hamming loss \",hamming_loss(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84cd949",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc63768",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMicro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMacro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6265cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report\")\n",
    "print (classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a65660",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf1 = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2abc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = ['Q1']\n",
    "te_confusions = [cf1]\n",
    "y_t = [y_test]\n",
    "te_probs = [pipe.predict_proba(x_test)]\n",
    "y_pred = [predictions]\n",
    "y_test_preds = predictions\n",
    "y_prob = [pipe.predict_proba(x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = pipe.predict_proba(x_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-.01, 1.])\n",
    "plt.ylim([-.01, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, cf, y_t, y_pred, y_prob in zip(Q, te_confusions, y_test, y_test_preds, te_probs):\n",
    "    make_confusion_matrix(cf, title='\\n'+title+' Confusion Matrix\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(row):\n",
    "    \"\"\"Subtracts the distance (in days) provided by the postD function.\n",
    "    Args:\n",
    "        row (record in the data): record\n",
    "\n",
    "    Returns:\n",
    "        datetime object: extract date - relative age = actual post date\n",
    "    \"\"\"\n",
    "    #np.argmax(row[['Q1_Prob', 'Q2_Prob','Q3_Prob','Q4_Prob']].values)\n",
    "    Q1 = row['Q1_probabilities'][1]\n",
    "    Q2 = row['Q2_probabilities'][1]\n",
    "    Q3 = row['Q3_probabilities'][1]\n",
    "    Q4 = row['Q4_probabilities'][1]\n",
    "    lst = [Q1,Q2,Q3,Q4]\n",
    "    # adding 1 so these correspond to the target names rather than 0 indexed.\n",
    "    return int(lst.index(max(lst))+1)\n",
    "    #next, check if that \n",
    "\n",
    "\n",
    "results_table['Q'] = results_table.apply( lambda row : Q(row), axis = 1)\n",
    "results_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6df773",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "#results_table = results_table.join(pd.DataFrame(ohe.fit_transform(results_table[['Q']]), columns=['Q1','Q2','Q3','Q4']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32953d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = ohe.fit_transform(results_table[['Q']]).T\n",
    "\n",
    "results_table['Q1'] = ohe[0]\n",
    "results_table['Q2'] = ohe[1]\n",
    "results_table['Q3'] = ohe[2]\n",
    "results_table['Q4'] = ohe[3]\n",
    "\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO CLean out all these nans Way upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92204e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table.to_csv(f'../app/data/tableau_table.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c781bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table=results_table[results_table.salary.notna()]\n",
    "results_table[results_table.Q != results_table.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8455974",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.quantile(results_table.salary, 0.25)\n",
    "q2 = np.quantile(results_table.salary, 0.5)\n",
    "q3 = np.quantile(results_table.salary, 0.75)\n",
    "\n",
    "# calc iqr\n",
    "iqr = (q3 - q1)\n",
    "# expand iqr to discern outliers\n",
    "iqr_x = iqr*1.5\n",
    "\n",
    "# setting the lower and upper limits\n",
    "iqr_lower = q1-iqr_x\n",
    "iqr_upper = q3+iqr_x\n",
    "\n",
    "\n",
    "sns.displot(results_table.salary)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = '#fd4d3f')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d3550",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table.Q.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94942b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"Q\", y=\"salary\", kind=\"violin\", inner=None, data=results_table)\n",
    "sns.swarmplot(x=\"Q\", y=\"salary\", color=\"k\", size=2, data=results_table, ax=g.ax)\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d190dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "city------------\n",
    "colmax == 'q3_PROB', probably taken from col name\n",
    "Company------------\n",
    "Date Posted----------------\n",
    "Description\n",
    "Focus\n",
    "Job title\n",
    "JobUrl\n",
    "Location\n",
    "Q\n",
    "Requirements\n",
    "Role\n",
    "Schedule\n",
    "State\n",
    "Summary\n",
    "Probability\n",
    "Q1 = 0 or 1 or null\n",
    "Q1_posts = 1.0 or 0.0\n",
    "Q1 pred = binary\n",
    "Q1 probs\n",
    "\n",
    "etc\n",
    "\n",
    "Salary\n",
    "\n",
    "\n",
    "top terms:\n",
    "Feature = values == term\n",
    "Q = class == 'Q1', etc\n",
    "Imortance == some float\n",
    "\n",
    "top states:\n",
    "the same but Feature = states, capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30765298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_tfidf = tfidf.fit_transform(_test).toarray()\n",
    "vocab = q1_vectorizer.vocabulary_\n",
    "reverse_vocab = {v:k for k,v in vocab.items()}\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "df_tfidf = pd.DataFrame(X_tfidf, columns = feature_names)\n",
    "\n",
    "idx = X_tfidf.argsort(axis=1)\n",
    "\n",
    "tfidf_max10 = idx[:,-10:]\n",
    "\n",
    "df_tfidf['top10'] = [[reverse_vocab.get(item) for item in row] for row in tfidf_max10 ]\n",
    "\n",
    "df_tfidf['top10']\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac83f05b37eb5c5c49ba67e50f0047ddfbc4b30205fc79ee5f327b9c0ac37f55"
  },
  "kernelspec": {
   "display_name": "Python [conda env:indeedapp]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
